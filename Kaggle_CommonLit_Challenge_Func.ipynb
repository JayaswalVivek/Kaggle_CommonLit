{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle CommonLit Readability Challenge\n",
    "Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, in_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=in_size, hidden_size=hidden_layer_size, bidirectional=True)\n",
    "        self.L1 = nn.Linear(2*hidden_layer_size, output_size[0])\n",
    "        self.L2 = nn.Linear(output_size[0], output_size[1])\n",
    "        self.L3 = nn.Linear(output_size[1], output_size[2])\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_output, (lstm_hn, lstm_cn) = self.lstm(input_seq)\n",
    "        \n",
    "        # Select the hidden state corresponding to the last element in the word sequence\n",
    "        # Since only one element is selected from the first index, in_matrix is a 2D tensor and not a 3D tensor\n",
    "        in_matrix = lstm_output[-1, :, :] \n",
    "\n",
    "        ffd_step_1 = torch.relu(self.L1(in_matrix))\n",
    "        ffd_step_2 = torch.relu(self.L2(ffd_step_1))\n",
    "        ffd_step_3 = self.L3(ffd_step_2)\n",
    "    \n",
    "        return ffd_step_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word embedding matrix for an excerpt\n",
    "def excerpt_embedding(in_excerpt, return_idx):\n",
    "    \n",
    "    nltk_word_tokens = nltk.wordpunct_tokenize(in_excerpt)\n",
    "    word_list_series = pd.Series(nltk_word_tokens)\n",
    "    rm_words = word_list_series.isin(spacy_stopwords)\n",
    "    retain_idx = np.where(~rm_words)[0]\n",
    "    word_list_series_no_stop_words = word_list_series[retain_idx]\n",
    "\n",
    "    num_words = len(retain_idx)\n",
    "    word_embedding = np.zeros((num_words, 50))\n",
    "\n",
    "    for idx in range(num_words):\n",
    "        current_word = word_list_series_no_stop_words.iloc[idx].lower()\n",
    "        try:\n",
    "            current_idx = keyset.index(current_word)\n",
    "            word_embedding[idx, :] = embeddings_dict[keyset[current_idx]]\n",
    "        except ValueError:\n",
    "            blank_var = 0\n",
    "            # print('Missing value = ', current_word)\n",
    "    \n",
    "    # Remove rows with all 0\n",
    "    row_sum = np.sum(word_embedding, axis=1)\n",
    "    non_zero_idx = np.where(row_sum != 0)[0]\n",
    "    word_embedding_mod = word_embedding[non_zero_idx, :].copy()\n",
    "\n",
    "    if return_idx == 0:\n",
    "        return word_embedding_mod.shape[0]\n",
    "    \n",
    "    return word_embedding_mod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
