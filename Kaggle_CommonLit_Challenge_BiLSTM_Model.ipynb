{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Kaggle_CommonLit_Challenge_Func.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Batch BiLSTM-based Predictive Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "words_per_essay = 77\n",
    "vec_len_per_word=50\n",
    "vec_len_hidden_layer=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the excerpt embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_embed = np.load('Embeddings_Valid_Excerpt.npy')\n",
    "essay_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_score_frame = pd.read_csv('Scores_Valid_Excerpt.csv')\n",
    "lit_score = torch.tensor(lit_score_frame['target'], dtype=torch.float) #dtype has to be float32 and not float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread of the 'target' values\n",
    "lit_score_frame['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_essays = lit_score_frame.shape[0]\n",
    "torch_essay = torch.zeros((words_per_essay, num_essays, vec_len_per_word))\n",
    "\n",
    "for idx in range(num_essays):\n",
    "    start_idx = 0 + idx*words_per_essay\n",
    "    end_idx = start_idx + words_per_essay\n",
    "    torch_essay[:, idx, :] = torch.tensor(essay_embed[start_idx:end_idx, :])\n",
    "    \n",
    "torch_essay.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "lit_model = BiLSTM(in_size=vec_len_per_word, hidden_layer_size=vec_len_hidden_layer, output_size=[30, 10, 1])\n",
    "lit_loss_function = nn.MSELoss()\n",
    "lit_optimizer = torch.optim.Adam(lit_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of model parameters\n",
    "sum(p.numel() for p in lit_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "loss_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "    \n",
    "    current_fit = lit_model(torch_essay)\n",
    "    current_loss = lit_loss_function(current_fit, torch.unsqueeze(lit_score, 1))  # 2\n",
    "    loss_per_epoch.append(current_loss.item())\n",
    "     \n",
    "    lit_optimizer.zero_grad()     # 3\n",
    "    current_loss.backward()       # 4\n",
    "    lit_optimizer.step()          # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(loss_per_epoch).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_itr_loss_frame = pd.DataFrame({'Itr':range(100), 'MSE_Loss':loss_per_epoch})\n",
    "per_itr_loss_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is split across multiple lines for readability and ease of modification\n",
    "loss_plot = ggplot(per_itr_loss_frame, aes(x='Itr', y='MSE_Loss'))\n",
    "loss_plot = loss_plot + geom_point() + geom_line() + scale_x_continuous(breaks=range(0, 100, 5))\n",
    "loss_plot = loss_plot + labs(title='MSE Loss Across Iterations', x='Iteration', y='MSE Loss')\n",
    "loss_plot = loss_plot + theme(plot_title=element_text(face='bold')\n",
    "                             , axis_title_x=element_text(face='plain', size=12)\n",
    "                             , axis_title_y=element_text(face='plain', size=12)\n",
    "                             , figure_size=(15, 5))\n",
    "\n",
    "loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = end_time - start_time\n",
    "time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time in minutes\n",
    "time_diff.total_seconds()/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-batch BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_batches = int(np.ceil(lit_score_frame.shape[0]/batch_size))\n",
    "print(num_batches)\n",
    "\n",
    "rng = np.random.default_rng(100)\n",
    "elements_per_batch = rng.choice(num_batches, lit_score_frame.shape[0], replace=True)\n",
    "np.unique(elements_per_batch, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70:30 split between training and test set\n",
    "0.7 * 2146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_count_per_batch = np.unique(elements_per_batch, return_counts=True)[1]\n",
    "np.min(np.where(idx_count_per_batch.cumsum() >= 1502)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trng_set_batches = list(range(0, 48))\n",
    "test_set_batches = list(range(48, 68))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "# lit_model = BiLSTM(in_size=vec_len_per_word, hidden_layer_size=vec_len_hidden_layer, output_size=[50, 1])\n",
    "lit_model = BiLSTM(in_size=vec_len_per_word, hidden_layer_size=vec_len_hidden_layer, output_size=[30, 10, 1])\n",
    "# lit_model = BiLSTM(in_size=vec_len_per_word, hidden_layer_size=vec_len_hidden_layer, output_size=[100, 50, 25, 1])\n",
    "lit_loss_function = nn.MSELoss()\n",
    "lit_optimizer = torch.optim.Adam(lit_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "test_loss_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch)\n",
    "    \n",
    "    for trng_batch_idx in trng_set_batches:\n",
    "        current_batch_idx = np.where(elements_per_batch == trng_batch_idx)[0]\n",
    "        current_fit = lit_model(torch_essay[:, current_batch_idx, :])\n",
    "        current_loss = lit_loss_function(current_fit, torch.unsqueeze(lit_score[current_batch_idx], 1))  # 2\n",
    "        lit_optimizer.zero_grad()     # 3\n",
    "        current_loss.backward()       # 4\n",
    "        lit_optimizer.step()          # 5\n",
    "        \n",
    "    temp_loss_per_batch = 0\n",
    "    for test_batch_idx in test_set_batches:\n",
    "        current_batch_idx = np.where(elements_per_batch == test_batch_idx)[0]\n",
    "        current_fit = lit_model(torch_essay[:, current_batch_idx, :])\n",
    "        current_loss = lit_loss_function(current_fit, torch.unsqueeze(lit_score[current_batch_idx], 1))  # 2\n",
    "        temp_loss_per_batch += current_loss.item()\n",
    "    \n",
    "    test_loss_per_epoch.append(temp_loss_per_batch/len(test_set_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(test_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.sqrt(test_loss_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_itr_test_loss_frame = pd.DataFrame({'Itr':range(20), 'MSE_Loss':test_loss_per_epoch})\n",
    "# Code is split across multiple lines for readability and ease of modification\n",
    "loss_plot = ggplot(per_itr_test_loss_frame, aes(x='Itr', y='MSE_Loss'))\n",
    "loss_plot = loss_plot + geom_point() + geom_line() + scale_x_continuous(breaks=range(0, 20, 1))\n",
    "loss_plot = loss_plot + labs(title='MSE Loss Across Iterations (Test Set)', x='Iteration', y='MSE Loss')\n",
    "loss_plot = loss_plot + theme(plot_title=element_text(face='bold')\n",
    "                             , axis_title_x=element_text(face='plain', size=12)\n",
    "                             , axis_title_y=element_text(face='plain', size=12)\n",
    "                             , figure_size=(15, 5))\n",
    "\n",
    "loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_2 = datetime.now()\n",
    "time_diff_2 = end_time_2 - start_time\n",
    "time_diff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time (in minutes)\n",
    "time_diff_2.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
